{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "\n",
    "tokenizer_folder = \"TokRoBERTa_BPE\"\n",
    "if os.path.isdir(tokenizer_folder) == False:\n",
    "    os.mkdir(tokenizer_folder)\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=[\"hi_dedup_1000.txt\"], vocab_size=32000, min_frequency=3, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TokRoBERTa_BPE/vocab.json', 'TokRoBERTa_BPE/merges.txt']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(tokenizer_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./TokRoBERTa_BPE/vocab.json\",\n",
    "    \"./TokRoBERTa_BPE/merges.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=41, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"рдорд╣рд╛рдкреНрд░рдмрдВрдзрдХ рднрд╛рд░рдд рд╕рдВрдЪрд╛рд░ рдирд┐рдЧрдо рд▓рд┐рдорд┐рдЯреЗрдб рджреБрд░реНрдЧ рдХреЛ рд╕рдореНрдкрддреНрддрд┐рдХрд░\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '├а┬д┬о├а┬д┬╣',\n",
       " '├а┬д┬╛',\n",
       " '├а┬д┬к',\n",
       " '├а┬е─п',\n",
       " '├а┬д┬░├а┬д┬м',\n",
       " '├а┬д─д',\n",
       " '├а┬д┬з├а┬д─╖',\n",
       " '─а├а┬д┼Г',\n",
       " '├а┬д┬╛',\n",
       " '├а┬д┬░├а┬д┬д',\n",
       " '─а├а┬д┬╕',\n",
       " '├а┬д─д',\n",
       " '├а┬д─╝',\n",
       " '├а┬д┬╛',\n",
       " '├а┬д┬░',\n",
       " '─а├а┬д┬и',\n",
       " '├а┬д┬┐',\n",
       " '├а┬д─╣├а┬д┬о',\n",
       " '─а├а┬д┬▓',\n",
       " '├а┬д┬┐',\n",
       " '├а┬д┬о',\n",
       " '├а┬д┬┐',\n",
       " '├а┬д┼Б',\n",
       " '├а┬е─й',\n",
       " '├а┬д┬б',\n",
       " '─а├а┬д┬ж',\n",
       " '├а┬е─г',\n",
       " '├а┬д┬░',\n",
       " '├а┬е─п',\n",
       " '├а┬д─╣',\n",
       " '─а├а┬д─╖',\n",
       " '├а┬е─н',\n",
       " '─а├а┬д┬╕├а┬д┬о',\n",
       " '├а┬е─п',\n",
       " '├а┬д┬к├а┬д┬д',\n",
       " '├а┬е─п',\n",
       " '├а┬д┬д',\n",
       " '├а┬д┬┐',\n",
       " '├а┬д─╖├а┬д┬░',\n",
       " '</s>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"рдорд╣рд╛рдкреНрд░рдмрдВрдзрдХ рднрд╛рд░рдд рд╕рдВрдЪрд╛рд░ рдирд┐рдЧрдо рд▓рд┐рдорд┐рдЯреЗрдб рджреБрд░реНрдЧ рдХреЛ рд╕рдореНрдкрддреНрддрд┐рдХрд░\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "TRAIN_EPOCHS = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEED = 42\n",
    "MAX_LEN = 128\n",
    "SUMMARY_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    # vocab_size=32000,\n",
    "    # max_position_embeddings=514,\n",
    "    # num_attention_heads=12,\n",
    "    # num_hidden_layers=6,\n",
    "    # type_vocab_size=1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    bos_token_id=0,\n",
    "    eos_token_id=2,\n",
    "    gradient_checkpointing=False,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    hidden_size=768,\n",
    "    initializer_range=0.02,\n",
    "    intermediate_size=3072,\n",
    "    layer_norm_eps=1e-05,\n",
    "    max_position_embeddings=514,\n",
    "    model_type=\"roberta\",\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    pad_token_id=1,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    transformers_version=\"4.4.0.dev0\",\n",
    "    type_vocab_size=1,\n",
    "    use_cache=True,\n",
    "    vocab_size=32000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./TokRoBERTa_BPE\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110651648"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dishantpadalia/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ЁЯдЧ Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./hi_dedup_1000.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./TokRoBERTa_BPE\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/Users/dishantpadalia/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16\n",
      "  Number of trainable parameters = 110651648\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c316d2e9594c79a0f474b4b23af1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 208.3559, 'train_samples_per_second': 4.799, 'train_steps_per_second': 0.077, 'train_loss': 8.832284927368164, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16, training_loss=8.832284927368164, metrics={'train_runtime': 208.3559, 'train_samples_per_second': 4.799, 'train_steps_per_second': 0.077, 'train_loss': 8.832284927368164, 'epoch': 1.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./TokRoBERTa_BPE/\n",
      "Configuration saved in ./TokRoBERTa_BPE/config.json\n",
      "Model weights saved in ./TokRoBERTa_BPE/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./TokRoBERTa_BPE/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./TokRoBERTa_BPE/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ./TokRoBERTa_BPE/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./TokRoBERTa_BPE/\",\n",
    "    tokenizer=\"./TokRoBERTa_BPE/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.017935452982783318,\n",
       "  'token': 264,\n",
       "  'token_str': 'рд╛',\n",
       "  'sequence': 'рд░рд╣рд╛ рд╣реИ рд╡рд╣ рди рд╕рд┐рд░реНрдл рдЪрд┐рдВрддрд╛рдЬрдирдХрд╛'},\n",
       " {'score': 0.005482870154082775,\n",
       "  'token': 267,\n",
       "  'token_str': 'реН',\n",
       "  'sequence': 'рд░рд╣рд╛ рд╣реИ рд╡рд╣ рди рд╕рд┐рд░реНрдл рдЪрд┐рдВрддрд╛рдЬрдирдХреН'},\n",
       " {'score': 0.004985986743122339,\n",
       "  'token': 265,\n",
       "  'token_str': 'реЗ',\n",
       "  'sequence': 'рд░рд╣рд╛ рд╣реИ рд╡рд╣ рди рд╕рд┐рд░реНрдл рдЪрд┐рдВрддрд╛рдЬрдирдХреЗ'},\n",
       " {'score': 0.002476259833201766,\n",
       "  'token': 270,\n",
       "  'token_str': 'рд┐',\n",
       "  'sequence': 'рд░рд╣рд╛ рд╣реИ рд╡рд╣ рди рд╕рд┐рд░реНрдл рдЪрд┐рдВрддрд╛рдЬрдирдХрд┐'},\n",
       " {'score': 0.0016431399853900075,\n",
       "  'token': 269,\n",
       "  'token_str': 'реА',\n",
       "  'sequence': 'рд░рд╣рд╛ рд╣реИ рд╡рд╣ рди рд╕рд┐рд░реНрдл рдЪрд┐рдВрддрд╛рдЬрдирдХреА'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"рд░рд╣рд╛ рд╣реИ рд╡рд╣ рди рд╕рд┐рд░реНрдл рдЪрд┐рдВрддрд╛рдЬрдирдХ <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.01747768558561802,\n",
       "  'token': 264,\n",
       "  'token_str': 'рд╛',\n",
       "  'sequence': 'рдкрд╣рд▓реВ рд╕рд╛рдордиреЗ рдЖрд╛ ред рдЕрдХреНрд╕рд░ рдХрдВрдкрдирд┐рдпреЛрдВ'},\n",
       " {'score': 0.005214308854192495,\n",
       "  'token': 267,\n",
       "  'token_str': 'реН',\n",
       "  'sequence': 'рдкрд╣рд▓реВ рд╕рд╛рдордиреЗ рдЖреН ред рдЕрдХреНрд╕рд░ рдХрдВрдкрдирд┐рдпреЛрдВ'},\n",
       " {'score': 0.005012022331357002,\n",
       "  'token': 265,\n",
       "  'token_str': 'реЗ',\n",
       "  'sequence': 'рдкрд╣рд▓реВ рд╕рд╛рдордиреЗ рдЖреЗ ред рдЕрдХреНрд╕рд░ рдХрдВрдкрдирд┐рдпреЛрдВ'},\n",
       " {'score': 0.0025437825825065374,\n",
       "  'token': 270,\n",
       "  'token_str': 'рд┐',\n",
       "  'sequence': 'рдкрд╣рд▓реВ рд╕рд╛рдордиреЗ рдЖрд┐ ред рдЕрдХреНрд╕рд░ рдХрдВрдкрдирд┐рдпреЛрдВ'},\n",
       " {'score': 0.0017022050451487303,\n",
       "  'token': 269,\n",
       "  'token_str': 'реА',\n",
       "  'sequence': 'рдкрд╣рд▓реВ рд╕рд╛рдордиреЗ рдЖреА ред рдЕрдХреНрд╕рд░ рдХрдВрдкрдирд┐рдпреЛрдВ'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"рдкрд╣рд▓реВ рд╕рд╛рдордиреЗ рдЖ <mask> ред рдЕрдХреНрд╕рд░ рдХрдВрдкрдирд┐рдпреЛрдВ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.018545446917414665,\n",
       "  'token': 264,\n",
       "  'token_str': 'рд╛',\n",
       "  'sequence': 'рдореИрдВ рдЙрдирд╕реЗ рдирдлрд░рдд рдХрд░рддреА рд╣реВрдБ рдЬрд┐рдиреНрд╣реЛрдВрдиреЗ рдХрд┐рд╕реА рдХреЛ рдЕрдЪреНрдЫреЗ рдХрд╛рдо рдХрд░рдиреЗ рд╕реЗ рд░реЛрдХрд╛ рд╣реИ. рдореИрдВ рдирд╣реАрдВ рдЬрд╛рдирддреА рдХрд┐ рдЗрд╕рдореЗрдВ рдХрд┐рддрдирд╛ рд╕рдордп рдФрд░ рд▓рдЧреЗрдЧрд╛, рдФрд░ рдореИрдВ рдпрд╣ рднреА рдирд╣реАрдВ рдЬрд╛рдирддреА рдХрд┐ рдореЗрд░реА рдорд╛рдВ рдШрд░ рд╡рд╛рдкрд╕ рдХрдм рдЖрдПрдБрдЧреА, рдореБрдЭреЗ рдбрд╛рдЯреЗрдВрдЧреА, рдореБрдЭреЗ рдЦрд╛рдирд╛ рдЦрд╛рдиреЗ рдХреЛ рдХрд╣реЗрдВрдЧреА, рдпрд╛ рдореБрдЭреЗрд╛'},\n",
       " {'score': 0.00518343411386013,\n",
       "  'token': 267,\n",
       "  'token_str': 'реН',\n",
       "  'sequence': 'рдореИрдВ рдЙрдирд╕реЗ рдирдлрд░рдд рдХрд░рддреА рд╣реВрдБ рдЬрд┐рдиреНрд╣реЛрдВрдиреЗ рдХрд┐рд╕реА рдХреЛ рдЕрдЪреНрдЫреЗ рдХрд╛рдо рдХрд░рдиреЗ рд╕реЗ рд░реЛрдХрд╛ рд╣реИ. рдореИрдВ рдирд╣реАрдВ рдЬрд╛рдирддреА рдХрд┐ рдЗрд╕рдореЗрдВ рдХрд┐рддрдирд╛ рд╕рдордп рдФрд░ рд▓рдЧреЗрдЧрд╛, рдФрд░ рдореИрдВ рдпрд╣ рднреА рдирд╣реАрдВ рдЬрд╛рдирддреА рдХрд┐ рдореЗрд░реА рдорд╛рдВ рдШрд░ рд╡рд╛рдкрд╕ рдХрдм рдЖрдПрдБрдЧреА, рдореБрдЭреЗ рдбрд╛рдЯреЗрдВрдЧреА, рдореБрдЭреЗ рдЦрд╛рдирд╛ рдЦрд╛рдиреЗ рдХреЛ рдХрд╣реЗрдВрдЧреА, рдпрд╛ рдореБрдЭреЗреН'},\n",
       " {'score': 0.004980894271284342,\n",
       "  'token': 265,\n",
       "  'token_str': 'реЗ',\n",
       "  'sequence': 'рдореИрдВ рдЙрдирд╕реЗ рдирдлрд░рдд рдХрд░рддреА рд╣реВрдБ рдЬрд┐рдиреНрд╣реЛрдВрдиреЗ рдХрд┐рд╕реА рдХреЛ рдЕрдЪреНрдЫреЗ рдХрд╛рдо рдХрд░рдиреЗ рд╕реЗ рд░реЛрдХрд╛ рд╣реИ. рдореИрдВ рдирд╣реАрдВ рдЬрд╛рдирддреА рдХрд┐ рдЗрд╕рдореЗрдВ рдХрд┐рддрдирд╛ рд╕рдордп рдФрд░ рд▓рдЧреЗрдЧрд╛, рдФрд░ рдореИрдВ рдпрд╣ рднреА рдирд╣реАрдВ рдЬрд╛рдирддреА рдХрд┐ рдореЗрд░реА рдорд╛рдВ рдШрд░ рд╡рд╛рдкрд╕ рдХрдм рдЖрдПрдБрдЧреА, рдореБрдЭреЗ рдбрд╛рдЯреЗрдВрдЧреА, рдореБрдЭреЗ рдЦрд╛рдирд╛ рдЦрд╛рдиреЗ рдХреЛ рдХрд╣реЗрдВрдЧреА, рдпрд╛ рдореБрдЭреЗреЗ'},\n",
       " {'score': 0.0024795588105916977,\n",
       "  'token': 270,\n",
       "  'token_str': 'рд┐',\n",
       "  'sequence': 'рдореИрдВ рдЙрдирд╕реЗ рдирдлрд░рдд рдХрд░рддреА рд╣реВрдБ рдЬрд┐рдиреНрд╣реЛрдВрдиреЗ рдХрд┐рд╕реА рдХреЛ рдЕрдЪреНрдЫреЗ рдХрд╛рдо рдХрд░рдиреЗ рд╕реЗ рд░реЛрдХрд╛ рд╣реИ. рдореИрдВ рдирд╣реАрдВ рдЬрд╛рдирддреА рдХрд┐ рдЗрд╕рдореЗрдВ рдХрд┐рддрдирд╛ рд╕рдордп рдФрд░ рд▓рдЧреЗрдЧрд╛, рдФрд░ рдореИрдВ рдпрд╣ рднреА рдирд╣реАрдВ рдЬрд╛рдирддреА рдХрд┐ рдореЗрд░реА рдорд╛рдВ рдШрд░ рд╡рд╛рдкрд╕ рдХрдм рдЖрдПрдБрдЧреА, рдореБрдЭреЗ рдбрд╛рдЯреЗрдВрдЧреА, рдореБрдЭреЗ рдЦрд╛рдирд╛ рдЦрд╛рдиреЗ рдХреЛ рдХрд╣реЗрдВрдЧреА, рдпрд╛ рдореБрдЭреЗрд┐'},\n",
       " {'score': 0.001678893924690783,\n",
       "  'token': 269,\n",
       "  'token_str': 'реА',\n",
       "  'sequence': 'рдореИрдВ рдЙрдирд╕реЗ рдирдлрд░рдд рдХрд░рддреА рд╣реВрдБ рдЬрд┐рдиреНрд╣реЛрдВрдиреЗ рдХрд┐рд╕реА рдХреЛ рдЕрдЪреНрдЫреЗ рдХрд╛рдо рдХрд░рдиреЗ рд╕реЗ рд░реЛрдХрд╛ рд╣реИ. рдореИрдВ рдирд╣реАрдВ рдЬрд╛рдирддреА рдХрд┐ рдЗрд╕рдореЗрдВ рдХрд┐рддрдирд╛ рд╕рдордп рдФрд░ рд▓рдЧреЗрдЧрд╛, рдФрд░ рдореИрдВ рдпрд╣ рднреА рдирд╣реАрдВ рдЬрд╛рдирддреА рдХрд┐ рдореЗрд░реА рдорд╛рдВ рдШрд░ рд╡рд╛рдкрд╕ рдХрдм рдЖрдПрдБрдЧреА, рдореБрдЭреЗ рдбрд╛рдЯреЗрдВрдЧреА, рдореБрдЭреЗ рдЦрд╛рдирд╛ рдЦрд╛рдиреЗ рдХреЛ рдХрд╣реЗрдВрдЧреА, рдпрд╛ рдореБрдЭреЗреА'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"рдореИрдВ рдЙрдирд╕реЗ рдирдлрд░рдд рдХрд░рддреА рд╣реВрдБ рдЬрд┐рдиреНрд╣реЛрдВрдиреЗ рдХрд┐рд╕реА рдХреЛ рдЕрдЪреНрдЫреЗ рдХрд╛рдо рдХрд░рдиреЗ рд╕реЗ рд░реЛрдХрд╛ рд╣реИ. рдореИрдВ рдирд╣реАрдВ рдЬрд╛рдирддреА рдХрд┐ рдЗрд╕рдореЗрдВ рдХрд┐рддрдирд╛ рд╕рдордп рдФрд░ рд▓рдЧреЗрдЧрд╛, рдФрд░ рдореИрдВ рдпрд╣ рднреА рдирд╣реАрдВ рдЬрд╛рдирддреА рдХрд┐ рдореЗрд░реА рдорд╛рдВ рдШрд░ рд╡рд╛рдкрд╕ рдХрдм рдЖрдПрдБрдЧреА, рдореБрдЭреЗ рдбрд╛рдЯреЗрдВрдЧреА, рдореБрдЭреЗ рдЦрд╛рдирд╛ рдЦрд╛рдиреЗ рдХреЛ рдХрд╣реЗрдВрдЧреА, рдпрд╛ рдореБрдЭреЗ <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: transformers-cli <command> [<args>]\n",
      "Transformers CLI tool: error: unrecognized arguments: ./TokRoBERTa_BPE\n"
     ]
    }
   ],
   "source": [
    "!transformers-cli add-new-model ./TokRoBERTa_BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f416df2e98a2debd3c815f38591d1eaeb62c97d7a01f9e221ecccaca550bc21b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
