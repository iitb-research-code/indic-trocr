{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d326de28baae4f4b9a52a6ccadc0e80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "\n",
    "tokenizer_folder = \"TokRoBERTa_BPE\"\n",
    "if os.path.isdir(tokenizer_folder) == False:\n",
    "    os.mkdir(tokenizer_folder)\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=[\"hi_dedup_1000.txt\"], vocab_size=32000, min_frequency=3, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TokRoBERTa_BPE/vocab.json', 'TokRoBERTa_BPE/merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(tokenizer_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./TokRoBERTa_BPE/vocab.json\",\n",
    "    \"./TokRoBERTa_BPE/merges.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=41, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"महाप्रबंधक भारत संचार निगम लिमिटेड दुर्ग को सम्पत्तिकर\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'à¤®à¤¹',\n",
       " 'à¤¾',\n",
       " 'à¤ª',\n",
       " 'à¥į',\n",
       " 'à¤°à¤¬',\n",
       " 'à¤Ĥ',\n",
       " 'à¤§à¤ķ',\n",
       " 'Ġà¤Ń',\n",
       " 'à¤¾',\n",
       " 'à¤°à¤¤',\n",
       " 'Ġà¤¸',\n",
       " 'à¤Ĥ',\n",
       " 'à¤ļ',\n",
       " 'à¤¾',\n",
       " 'à¤°',\n",
       " 'Ġà¤¨',\n",
       " 'à¤¿',\n",
       " 'à¤Ĺà¤®',\n",
       " 'Ġà¤²',\n",
       " 'à¤¿',\n",
       " 'à¤®',\n",
       " 'à¤¿',\n",
       " 'à¤Ł',\n",
       " 'à¥ĩ',\n",
       " 'à¤¡',\n",
       " 'Ġà¤¦',\n",
       " 'à¥ģ',\n",
       " 'à¤°',\n",
       " 'à¥į',\n",
       " 'à¤Ĺ',\n",
       " 'Ġà¤ķ',\n",
       " 'à¥ĭ',\n",
       " 'Ġà¤¸à¤®',\n",
       " 'à¥į',\n",
       " 'à¤ªà¤¤',\n",
       " 'à¥į',\n",
       " 'à¤¤',\n",
       " 'à¤¿',\n",
       " 'à¤ķà¤°',\n",
       " '</s>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"महाप्रबंधक भारत संचार निगम लिमिटेड दुर्ग को सम्पत्तिकर\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "TRAIN_EPOCHS = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEED = 42\n",
    "MAX_LEN = 128\n",
    "SUMMARY_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    # vocab_size=32000,\n",
    "    # max_position_embeddings=514,\n",
    "    # num_attention_heads=12,\n",
    "    # num_hidden_layers=6,\n",
    "    # type_vocab_size=1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    bos_token_id=0,\n",
    "    eos_token_id=2,\n",
    "    gradient_checkpointing=False,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    hidden_size=768,\n",
    "    initializer_range=0.02,\n",
    "    intermediate_size=3072,\n",
    "    layer_norm_eps=1e-05,\n",
    "    max_position_embeddings=514,\n",
    "    model_type=\"roberta\",\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    pad_token_id=1,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    transformers_version=\"4.4.0.dev0\",\n",
    "    type_vocab_size=1,\n",
    "    use_cache=True,\n",
    "    vocab_size=32000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./TokRoBERTa_BPE\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110651648"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dishantpadalia/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./hi_dedup_1000.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./TokRoBERTa_BPE\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/Users/dishantpadalia/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16\n",
      "  Number of trainable parameters = 110651648\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf754d86adb498983189bbfb76d56e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 218.4866, 'train_samples_per_second': 4.577, 'train_steps_per_second': 0.073, 'train_loss': 8.77268123626709, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16, training_loss=8.77268123626709, metrics={'train_runtime': 218.4866, 'train_samples_per_second': 4.577, 'train_steps_per_second': 0.073, 'train_loss': 8.77268123626709, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./TokRoBERTa_BPE/\n",
      "Configuration saved in ./TokRoBERTa_BPE/config.json\n",
      "Model weights saved in ./TokRoBERTa_BPE/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./TokRoBERTa_BPE/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\ngit init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mpush_to_hub()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/transformers/trainer.py:3456\u001b[0m, in \u001b[0;36mTrainer.push_to_hub\u001b[0;34m(self, commit_message, blocking, **kwargs)\u001b[0m\n\u001b[1;32m   3453\u001b[0m \u001b[39m# If a user calls manually `push_to_hub` with `self.args.push_to_hub = False`, we try to create the repo but\u001b[39;00m\n\u001b[1;32m   3454\u001b[0m \u001b[39m# it might fail.\u001b[39;00m\n\u001b[1;32m   3455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrepo\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 3456\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_git_repo()\n\u001b[1;32m   3458\u001b[0m model_name \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   3459\u001b[0m \u001b[39mif\u001b[39;00m model_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/transformers/trainer.py:3309\u001b[0m, in \u001b[0;36mTrainer.init_git_repo\u001b[0;34m(self, at_init)\u001b[0m\n\u001b[1;32m   3306\u001b[0m     repo_name \u001b[39m=\u001b[39m get_full_repo_name(repo_name, token\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_token)\n\u001b[1;32m   3308\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3309\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepo \u001b[39m=\u001b[39m Repository(\n\u001b[1;32m   3310\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49moutput_dir,\n\u001b[1;32m   3311\u001b[0m         clone_from\u001b[39m=\u001b[39;49mrepo_name,\n\u001b[1;32m   3312\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   3313\u001b[0m         private\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_private_repo,\n\u001b[1;32m   3314\u001b[0m     )\n\u001b[1;32m   3315\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m   3316\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39moverwrite_output_dir \u001b[39mand\u001b[39;00m at_init:\n\u001b[1;32m   3317\u001b[0m         \u001b[39m# Try again after wiping output_dir\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message, \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/huggingface_hub/repository.py:528\u001b[0m, in \u001b[0;36mRepository.__init__\u001b[0;34m(self, local_dir, clone_from, repo_type, token, git_user, git_email, revision, private, skip_lfs_files, client)\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhuggingface_token \u001b[39m=\u001b[39m HfFolder\u001b[39m.\u001b[39mget_token()\n\u001b[1;32m    527\u001b[0m \u001b[39mif\u001b[39;00m clone_from \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 528\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclone_from(repo_url\u001b[39m=\u001b[39;49mclone_from)\n\u001b[1;32m    529\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m     \u001b[39mif\u001b[39;00m is_git_repo(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_dir):\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/huggingface_hub/repository.py:762\u001b[0m, in \u001b[0;36mRepository.clone_from\u001b[0;34m(self, repo_url, token)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m     \u001b[39m# Check if the folder is the root of a git repository\u001b[39;00m\n\u001b[1;32m    761\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_git_repo(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_dir):\n\u001b[0;32m--> 762\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    763\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTried to clone a repository in a non-empty folder that isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    764\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m git repository. If you really want to do this, do it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    765\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m manually:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mgit init && git remote add origin && git pull\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m origin main\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m or clone repo to a new folder and move your\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    767\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m existing files there afterwards.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    768\u001b[0m         )\n\u001b[1;32m    770\u001b[0m     \u001b[39mif\u001b[39;00m is_local_clone(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_dir, repo_url):\n\u001b[1;32m    771\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    772\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocal_dir\u001b[39m}\u001b[39;00m\u001b[39m is already a clone of \u001b[39m\u001b[39m{\u001b[39;00mclean_repo_url\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    773\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m Make sure you pull the latest changes with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    774\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `repo.git_pull()`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    775\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\ngit init && git remote add origin && git pull origin main\n or clone repo to a new folder and move your existing files there afterwards."
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./TokRoBERTa_BPE/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ./TokRoBERTa_BPE/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./TokRoBERTa_BPE/\",\n",
    "    tokenizer=\"./TokRoBERTa_BPE/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.017935452982783318,\n",
       "  'token': 264,\n",
       "  'token_str': 'ा',\n",
       "  'sequence': 'रहा है वह न सिर्फ चिंताजनका'},\n",
       " {'score': 0.005482870154082775,\n",
       "  'token': 267,\n",
       "  'token_str': '्',\n",
       "  'sequence': 'रहा है वह न सिर्फ चिंताजनक्'},\n",
       " {'score': 0.004985986743122339,\n",
       "  'token': 265,\n",
       "  'token_str': 'े',\n",
       "  'sequence': 'रहा है वह न सिर्फ चिंताजनके'},\n",
       " {'score': 0.002476259833201766,\n",
       "  'token': 270,\n",
       "  'token_str': 'ि',\n",
       "  'sequence': 'रहा है वह न सिर्फ चिंताजनकि'},\n",
       " {'score': 0.0016431399853900075,\n",
       "  'token': 269,\n",
       "  'token_str': 'ी',\n",
       "  'sequence': 'रहा है वह न सिर्फ चिंताजनकी'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"रहा है वह न सिर्फ चिंताजनक <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.01747768558561802,\n",
       "  'token': 264,\n",
       "  'token_str': 'ा',\n",
       "  'sequence': 'पहलू सामने आा । अक्सर कंपनियों'},\n",
       " {'score': 0.005214308854192495,\n",
       "  'token': 267,\n",
       "  'token_str': '्',\n",
       "  'sequence': 'पहलू सामने आ् । अक्सर कंपनियों'},\n",
       " {'score': 0.005012022331357002,\n",
       "  'token': 265,\n",
       "  'token_str': 'े',\n",
       "  'sequence': 'पहलू सामने आे । अक्सर कंपनियों'},\n",
       " {'score': 0.0025437825825065374,\n",
       "  'token': 270,\n",
       "  'token_str': 'ि',\n",
       "  'sequence': 'पहलू सामने आि । अक्सर कंपनियों'},\n",
       " {'score': 0.0017022050451487303,\n",
       "  'token': 269,\n",
       "  'token_str': 'ी',\n",
       "  'sequence': 'पहलू सामने आी । अक्सर कंपनियों'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"पहलू सामने आ <mask> । अक्सर कंपनियों\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.018545446917414665,\n",
       "  'token': 264,\n",
       "  'token_str': 'ा',\n",
       "  'sequence': 'मैं उनसे नफरत करती हूँ जिन्होंने किसी को अच्छे काम करने से रोका है. मैं नहीं जानती कि इसमें कितना समय और लगेगा, और मैं यह भी नहीं जानती कि मेरी मां घर वापस कब आएँगी, मुझे डाटेंगी, मुझे खाना खाने को कहेंगी, या मुझेा'},\n",
       " {'score': 0.00518343411386013,\n",
       "  'token': 267,\n",
       "  'token_str': '्',\n",
       "  'sequence': 'मैं उनसे नफरत करती हूँ जिन्होंने किसी को अच्छे काम करने से रोका है. मैं नहीं जानती कि इसमें कितना समय और लगेगा, और मैं यह भी नहीं जानती कि मेरी मां घर वापस कब आएँगी, मुझे डाटेंगी, मुझे खाना खाने को कहेंगी, या मुझे्'},\n",
       " {'score': 0.004980894271284342,\n",
       "  'token': 265,\n",
       "  'token_str': 'े',\n",
       "  'sequence': 'मैं उनसे नफरत करती हूँ जिन्होंने किसी को अच्छे काम करने से रोका है. मैं नहीं जानती कि इसमें कितना समय और लगेगा, और मैं यह भी नहीं जानती कि मेरी मां घर वापस कब आएँगी, मुझे डाटेंगी, मुझे खाना खाने को कहेंगी, या मुझेे'},\n",
       " {'score': 0.0024795588105916977,\n",
       "  'token': 270,\n",
       "  'token_str': 'ि',\n",
       "  'sequence': 'मैं उनसे नफरत करती हूँ जिन्होंने किसी को अच्छे काम करने से रोका है. मैं नहीं जानती कि इसमें कितना समय और लगेगा, और मैं यह भी नहीं जानती कि मेरी मां घर वापस कब आएँगी, मुझे डाटेंगी, मुझे खाना खाने को कहेंगी, या मुझेि'},\n",
       " {'score': 0.001678893924690783,\n",
       "  'token': 269,\n",
       "  'token_str': 'ी',\n",
       "  'sequence': 'मैं उनसे नफरत करती हूँ जिन्होंने किसी को अच्छे काम करने से रोका है. मैं नहीं जानती कि इसमें कितना समय और लगेगा, और मैं यह भी नहीं जानती कि मेरी मां घर वापस कब आएँगी, मुझे डाटेंगी, मुझे खाना खाने को कहेंगी, या मुझेी'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"मैं उनसे नफरत करती हूँ जिन्होंने किसी को अच्छे काम करने से रोका है. मैं नहीं जानती कि इसमें कितना समय और लगेगा, और मैं यह भी नहीं जानती कि मेरी मां घर वापस कब आएँगी, मुझे डाटेंगी, मुझे खाना खाने को कहेंगी, या मुझे <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: transformers-cli <command> [<args>]\n",
      "Transformers CLI tool: error: unrecognized arguments: ./TokRoBERTa_BPE\n"
     ]
    }
   ],
   "source": [
    "!transformers-cli add-new-model ./TokRoBERTa_BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f416df2e98a2debd3c815f38591d1eaeb62c97d7a01f9e221ecccaca550bc21b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
